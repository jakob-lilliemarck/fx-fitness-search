{% extends "index.html" %} {% block content %}
<article>
    <header>
        <div class="meta">
            <span class="author">{{ author }}</span>
            <span class="date">{{ self.publication_date() }}</span>
        </div>
    </header>
    <section>
        <h1>{{ heading }}</h1>
        <p>
            I've spent the better part of this Autumn thinking about and
            experimenting with machine learning in Rust. Initially I was curious
            if it would be feasible for me to do, and what it would take in
            terms of computational, not to mention cognitive resources - would
            it be within my reach?
        </p>
        <p>
            "Why Rust?", you may ask. Why not Python, or even C? I have been
            asking myself the same question, but truth be told, I find neither
            of which compelling to work with. After all, in most systems (yes,
            even machine learning ones) the majority of the code is not about
            tensors and models, but about data retrieval and preprocessing. If
            you ask me, Rust has it all - excellent support for test driven
            development, memory safety, robust and explicit parallel computing
            and a proper expressive type system. I'd even argue that Rust is a
            simple language because in Rust there are no surprises and there is
            no magic.
        </p>
    </section>

    <section>
        <h2>The Morphological Space of Neural Networks</h2>
        <p>
            I've learned that while doing machine learning, there are a lot of
            decisions to make. How to handle missing data? How to process,
            extract and structure data in a form the models and frameworks will
            accept? Which model, architecture and training parameters to use,
            and for which use-case? The list goes on.
        </p>
        <p>
            From memory, most examples or articles I've seen in the past about
            machine learning (though I must admit I really haven't done my
            homework here) typically involve an enthusiastic author reasoning
            about the validity and meaning of parameters while making one-off
            experiments. That makes me itch. If there's anything equivalent to
            "code smell" for data science, I think that's it—you've got these
            sophisticated models producing measurable predictions, and yet you
            allow your human superstition and poor analytical skills to dictate
            the conditions of your experiments. I can't but ask "why?".
        </p>
        <p>
            Consider a simple model, a feedforward neural network. Even this
            seemingly straightforward architecture exists within a vast space of
            design choices. To illustrate the scale of this space, its
            "morphology" if you will, let me discretize the key dimensions that
            impact model performance:
        </p>
        <table>
            <thead>
                <tr>
                    <th>Variable</th>
                    <th>Choices</th>
                    <th>Count</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Sequence length</td>
                    <td>5, 10, 20, 30, 50</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Hidden size</td>
                    <td>32, 64, 128, 256, 512</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Activation function</td>
                    <td>ReLU, Tanh, Sigmoid</td>
                    <td>3</td>
                </tr>
                <tr>
                    <td>Epochs</td>
                    <td>10, 25, 50, 100, 200</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Batch size</td>
                    <td>16, 32, 64, 128, 256</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Learning rate</td>
                    <td>1e-4, 1e-3, 1e-2, 1e-1</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Optimizer type</td>
                    <td>Adam, SGD, RMSprop</td>
                    <td>3</td>
                </tr>
                <tr>
                    <td>Gradient clipping</td>
                    <td>None, 0.5, 1.0, 2.0</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Weight decay</td>
                    <td>None, 1e-4, 1e-3, 1e-2</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Patience</td>
                    <td>None, 5, 10, 20</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Model initialization</td>
                    <td>Default, Xavier, He</td>
                    <td>3</td>
                </tr>
            </tbody>
        </table>
        <p>
            That is
            <code>5 × 5 × 3 × 5 × 5 × 4 × 3 × 4 × 4 × 4 × 3 = ~864,000</code>
            possible configurations before even considering the input data or
            its preprocessing!
        </p>
        <p>
            Even for a simple feedforward network, the morphological space is
            massive! Any approach relying on manual experimentation to search
            such a vast space is at best going to be impractical - this is a
            problem that calls for a structured quantitative approach.
        </p>
    </section>

    <section>
        <h2>Making sense of the mess</h2>
        <p>
            In a previous job of mine, I worked in an innovation hub at one of
            the largest architectural firms in Sweden. My role at the time was
            as computational designer, an odd term that often provided people
            with more questions than answers. As a job title it was not ideal,
            but it was an interesting job. It involved applying programming,
            algorithms and systems thinking to the processes of creative
            exploration and manufacturing of complex three dimensional forms.
            The tool I used allowed for geometric optimization by means of a
            built in genetic algorithm solver. Genetic algorithms (GA) is an
            idea that takes its inspiration from Darwinian evolution theory and
            employs operations such as "mutation" and "crossover" to find good
            candidate solutions. The hypothesis is that two fit solutions are
            likely to have fit offspring. As it turns out GA is also a commonly
            used tool for optimizing machine learning systems.
        </p>
        <p>
            A few weeks back, and with all of the above in mind, I wrote a
            simple event driven GA library with Postgres persistence for Rust. I
            will not be going in to the technical details of it now, however I
            welcome you to check it out, should you be interested:
        </p>
        <a href="https://github.com/jakob-lilliemarck/fx-durable-ga"
            >https://github.com/jakob-lilliemarck/fx-durable-ga
        </a>
        <p>The question on my mind was:</p>
        <blockquote>
            Could genetic algorithms be a way to search for "predictive ability"
            over feature selection, preprocessing, model architecture and
            training parameters combined?
        </blockquote>
        <p>
            Searching for a good solution across all of those is after all the
            reality of training just about any model, on any dataset. Surely
            that space is bound to be so large that it's impractical, improbable
            and perhaps even impossible, for any manual process to find a global
            optimum, or even a good local one. As a developer I've come to know
            my human weaknesses well enough that I do not want to pick those
            parameters by hand, but perhaps more importantly: I do not care
            about the parameters, I care about the result.
        </p>
        <p>
            In search of an answer to my question I set up a project to attempt
            a combined search for predictive ability using the "Beijing
            Multi-Site Air Quality" dataset (Chen, 2017) while making use of my
            new library:
        </p>
        <a href="https://github.com/jakob-lilliemarck/fx-fitness-search"
            >https://github.com/jakob-lilliemarck/fx-fitness-search
        </a>
    </section>

    <section>
        <h2>Genetic encoding and fitness</h2>
        <p>
            So how does one get a genetic algorithm to explore a landscape
            solutions?
        </p>
        <p>
            In an attempt to shed some light, lets for a moment consider a much
            smaller and much simpler problem. Let's imagine that we're searching
            for a point <var>A</var> within a cube, and the closer we get to it,
            the better is our solution.
        </p>
        <p>
            We represent the cube using the cartesian coordinate system, within
            which it occupies some space along each of the three axes
            <var>X</var>, <var>Y</var> and <var>Z</var>. The geometric bounds of
            the cube could be defined as a maximum and a minimum value along
            each axis. For simplicities sake, let's say the cube occupies the
            space between the origin point <code>[0.0, 0.0, 0.0]</code> and a
            point <code>[1.0, 1.0, 1.0]</code>. The full cube could then be
            represented as the following.
        </p>
        <pre><code class="language-javascript">// X, Y and Z axis
[
  { min: 0.0, max: 1.0 },
  { min: 0.0, max: 1.0 },
  { min: 0.0, max: 1.0 }
]</code></pre>
        <p>
            Knowing that we're searching for a point <i>within</i> the cube we
            can now discard the infinite numbers of points that lie outside it,
            and focus our attention to the much smaller infinite number of
            points that lie within it. In other words, we know the valid range
            of values for <var>X</var>, <var>Y</var> and <var>Z</var>, the
            morphology of our search space.
        </p>
        <p>
            To compute the <i>fitness</i> of a candidate, we measure its
            distance to point <code>A</code>. Points closer to
            <code>A</code> will have a lower fitness, while points further a
            away will have a higher fitness. Our optimization will strive to
            minimize fitness.
        </p>
        <p>
            Now we mostly have what we need to generate candidate solutions. For
            reasons beyond this example, the framework I've written requires
            genes to be integers, so in order to adapt this example and adhere
            to those requirements we'll also need to define the number of steps,
            the resolution, along each axis. That limits the search space
            further and makes the resolution explicit.
        </p>
        <p>
            Every valid point along an axis like
            <code>{ min: 0.0, max: 1.0, steps: 11 }</code>
            could then be described as one of the 11 integer numbers between 0
            and 10. We now possess the means of encoding and decoding a point
            through the morphological bounds of the cube. We call the decoded
            form the <i>phenotype</i>, a term from genetics encompassing the
            observable traits of an organism, and the encoded form the
            <i>genotype</i>, the genetic information of that organism. To give
            an example:
        </p>
        <pre><code class="language-javascript">// Morphology, the space we search solutions within
[
  { min: 0.0, max: 1.0, steps: 11 },
  { min: 0.0, max: 1.0, steps: 11 },
  { min: 0.0, max: 1.0, steps: 11 }
]

// Point candidate solution, phenotype representation
{
  x: 0.2,
  y: 0.1,
  z: 0.9
}

// Candidate solution, encoded to genotype representation
[
  2, // 0.2 * (11 - 1) = 0.2 * 10 = 2,
  1, // 0.1 * (11 - 1) = 0.1 * 10 = 1
  9  // 0.9 * (11 - 1) = 0.9 * 10 = 9
]</code></pre>
        <p>
            This business of encoding and decoding may feel like jumping through
            hoops. However this transformation allows us to represent both
            integer and decimal numbers, booleans and enumerations, which
            together allows for expressing most conceivable search spaces while
            allowing the framework to handle any gene of any search space in a
            uniform manner. Using the morphological definition, the GA framework
            can now <code>generate</code>, <code>breed</code>,
            <code>mutate</code> and <code>evaluate</code> phenotypes. That is
            the foundation of a GA optimization solver.
        </p>
    </section>

    <section>
        <h2>Putting it in context</h2>
        <p>
            Looking at the dataset and considering my question I drew three
            conclusions.
        </p>
        <p class="conclusion">
            I do not know which data points have a strong or weak correlation
            and which would provide valuable information and which would just
            provide noise. Therefore I want the system to freely be able to
            select an arbitrary number of data points from the input dataset.
        </p>
        <p class="conclusion">
            I can not determine which preprocessing methods are suitable to
            extract information from which data points. Therefore I want the
            system to freely be able to select up to a predefined number of
            preprocessing methods for each data point.
        </p>
        <p class="conclusion">
            The relationships between the selection of data points,
            preprocessors, training parameters and model architecture may hold
            valuable synergies. Any attempt at a stepwise optimization, for
            example first optimizing model architecture, and then optimizing
            training parameters, will hide these synergies from the system.
            Therefore I want to optimize them all at once, in a
            <i>combined</i> search for fitness.
        </p>
        <p>
            But representing optional values within our GA poses a new challenge
            as the genotype encoding relies on the position of genes. So how to
            represent those "arbitrary number of" or "up to" expressions?
        </p>
        <p>
            What if we used 2 genes to "wrap" the genes encoding a specific
            attribute? Something like:
        </p>
        <ul>
            <li>
                One gene bounded at 0 and 1 with 2 steps, encoding if the
                wrapped "block" is active or not.
            </li>
            <li>
                One gene bounded between 0 and the number of options (column
                names, preprocessor types, etc.), acting like a "selector".
            </li>
        </ul>
        <pre><code class="language-javascript">[
    // The on/off "block toggle"
    { min: 0.0, max: 1.0, steps: 2 },
    // The "column selector", there are 12 columns of interest
    { min: 0.0, max: 11.0, steps: 12 },
    // .. the wrapped gene(s) follow here
]</code></pre>
        <p>
            This design comes with the constraint that the wrapped genes must be
            of equal length and must share the same number of options per gene.
            While it didn't feel ideal, I decided it was good enough for now.
        </p>
    </section>
    <section>
        <h2>The experiment</h2>
        <p>TBD</p>
        {{ chart_html|safe }}
        <p>TBD</p>
    </section>
</article>
{% endblock %}
