<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Signal in the noise</title>
        <style>
            :root {
                --primary: #4800b0; /*#ff0080*/
                --text: #222;
                --font-md: 1rem;
                --font-sm: 0.875rem;
                --spacing: 0.25rem;
            }
            html,
            body {
                font-size: var(--font-md);
                font-family:
                    -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                    "Helvetica Neue", Arial, sans-serif;
                line-height: 1.65;
                color: var(--text);
                display: flex;
                flex: 1 1;
                flex-direction: column;
                justify-content: center;
                margin: 0;
                padding: 0;
            }
            body {
                max-width: 900px;
                margin: auto;
            }
            article {
                display: flex;
                flex-direction: column;
                padding: calc(16 * var(--spacing)) calc(8 * var(--spacing));
            }
            article > header {
                margin-bottom: calc(8 * var(--spacing));
                padding-bottom: calc(4 * var(--spacing));
                display: flex;
                flex-direction: column;
            }
            .meta {
                display: flex;
                flex-direction: column;
                align-items: flex-end;
                gap: calc(1 * var(--spacing));
                margin-bottom: calc(2 * var(--spacing));
                font-size: var(--font-sm);
                color: #999;
                text-align: right;
            }
            .meta .author,
            .meta .date {
                display: block;
            }
            section {
                display: flex;
                flex-direction: column;
            }
            section + section {
                border-top: 1px solid #eaeaea;
                margin-top: calc(8 * var(--spacing));
                padding-top: calc(6 * var(--spacing));
            }
            h1,
            h2,
            h3,
            p {
                margin: 0;
            }
            h1 {
                font-size: 2.5rem;
                line-height: 1.2;
                margin-bottom: calc(4 * var(--spacing));
            }
            h2 {
                font-size: 1.75rem;
                line-height: 1.3;
            }
            p + p {
                margin-top: calc(3 * var(--spacing));
            }
            a {
                color: var(--primary);
                text-decoration: none;
            }
            a:hover {
                text-decoration: underline;
            }
            table {
                border-collapse: collapse;
                margin: 1rem 0;
            }
            th,
            td {
                border: 1px solid #ddd;
                padding: 0.5rem 0.75rem;
                text-align: left;
            }
            th {
                background-color: #f5f5f5;
                font-weight: bold;
            }
            blockquote {
                background-color: #f9f9f9;
                border-left: 4px solid var(--primary);
                margin: 1rem 0;
                padding: 1rem 1.5rem;
                font-style: italic;
            }
            var,
            code {
                background-color: #f5f5f5;
                color: var(--primary);
                padding: 0.125rem 0.375rem;
                border-radius: var(--spacing);
                font-family: "Courier New", monospace;
                font-size: var(--font-sm);
            }
            pre {
                background-color: #f5f5f5;
                padding: 1rem;
                border-radius: calc(2 * var(--spacing));
                overflow-x: auto;
                margin: calc(3 * var(--spacing)) 0;
                box-sizing: border-box;
                max-width: 100%;
            }
            canvas {
                max-width: 100%;
                height: auto;
            }
            pre code {
                background-color: transparent;
                color: inherit;
                padding: 0;
                border-radius: 0;
            }
            /* Mobile optimizations */
            @media (max-width: 768px) {
                :root {
                    --font-md: 0.9rem;
                    --font-sm: 0.8rem;
                }
                article {
                    padding: calc(8 * var(--spacing)) calc(2 * var(--spacing));
                }
                h1 {
                    font-size: 2rem;
                }
                h2 {
                    font-size: 1.5rem;
                }
                pre,
                code {
                    font-size: 0.75rem;
                }
                pre {
                    max-width: calc(100vw - calc(4 * var(--spacing)));
                }
            }
        </style>
        <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
        <!-- For Prism.js with Night Owl theme -->
        <link
            href="https://cdn.jsdelivr.net/npm/prism-themes@1.9.0/themes/prism-night-owl.min.css"
            rel="stylesheet"
        />
        <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    </head>
    <body>
        
<article>
    <header>
        <div class="meta">
            <span class="author">Jakob Lilliemarck</span>
            <span class="date">December 2025</span>
        </div>
    </header>
    <section>
        <h1>In search of predictive ability</h1>
        <p>
            I've spent the better part of this Autumn thinking about and
            experimenting with machine learning in Rust. Initially I was curious
            if it would be feasible for me to do, and what it would take in
            terms of computational, not to mention cognitive resources - would
            it be within my reach?
        </p>
        <p>
            "Why Rust?", you may ask. Why not Python, or even C? I have been
            asking myself the same question, but truth be told, I find neither
            of which compelling to work with. After all, in most systems (yes,
            even machine learning ones) the majority of the code is not about
            tensors and models, but about data retrieval and preprocessing. If
            you ask me, Rust has it all - excellent support for test driven
            development, memory safety, robust and explicit parallel computing
            and a proper expressive type system. I'd even argue that Rust is a
            simple language because in Rust there are no surprises and there is
            no magic.
        </p>
    </section>

    <section>
        <h2>The Morphological Space of Neural Networks</h2>
        <p>
            I've learned that while doing machine learning, there are a lot of
            decisions to make. How to handle missing data? How to process,
            extract and structure data in a form the models and frameworks will
            accept? Which model, architecture and training parameters to use,
            and for which use-case? The list goes on.
        </p>
        <p>
            From memory, most examples or articles I've seen about machine
            learning (though I must admit I really haven't done my homework
            here) typically involve an enthusiastic author reasoning about the
            validity and meaning of parameters while making one-off experiments.
            That makes me itch. If there's any equivalent to "code smell" for
            data science, I think that's it—you've got these sophisticated
            models producing measurable predictions, and yet you allow your
            human superstition and poor analytical skills to dictate the
            conditions of your experiments. I can't but ask "why?".
        </p>
        <p>
            Consider a simple model, a feedforward neural network. Even this
            seemingly straightforward architecture exists within a vast space of
            design choices. To illustrate the scale of this space, its
            "morphology" if you will, let me discretize the key dimensions that
            impact model performance:
        </p>
        <table>
            <thead>
                <tr>
                    <th>Variable</th>
                    <th>Choices</th>
                    <th>Count</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Sequence length</td>
                    <td>5, 10, 20, 30, 50</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Hidden size</td>
                    <td>32, 64, 128, 256, 512</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Activation function</td>
                    <td>ReLU, Tanh, Sigmoid</td>
                    <td>3</td>
                </tr>
                <tr>
                    <td>Epochs</td>
                    <td>10, 25, 50, 100, 200</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Batch size</td>
                    <td>16, 32, 64, 128, 256</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Learning rate</td>
                    <td>1e-4, 1e-3, 1e-2, 1e-1</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Optimizer type</td>
                    <td>Adam, SGD, RMSprop</td>
                    <td>3</td>
                </tr>
                <tr>
                    <td>Gradient clipping</td>
                    <td>None, 0.5, 1.0, 2.0</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Weight decay</td>
                    <td>None, 1e-4, 1e-3, 1e-2</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Patience</td>
                    <td>None, 5, 10, 20</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Model initialization</td>
                    <td>Default, Xavier, He</td>
                    <td>3</td>
                </tr>
            </tbody>
        </table>
        <p>
            That is
            <code>5 × 5 × 3 × 5 × 5 × 4 × 3 × 4 × 4 × 4 × 3 = ~864,000</code>
            possible configurations before even considering the input data or
            its preprocessing!
        </p>
        <p>
            Even for a simple feedforward network, the morphological space is
            massive! Any approach relying on manual experimentation to search
            such a vast space is at best going to be impractical - this is a
            problem that calls for a structured quantitative approach.
        </p>
    </section>

    <section>
        <h2>Making sense of the mess</h2>
        <p>
            In a previous job of mine, I worked in an innovation hub at one of
            the largest architectural firms in Sweden. My role at the time, was
            as computational designer, an odd term that often provided people
            with more questions than answers. As a job title it was not ideal,
            but it was an interesting job. It involved applying programmatic
            methods for exploring and manufacturing complex three dimensional
            forms. The tool I used allowed for geometric optimization by means
            of a built in genetic algorithm solver. Genetic algorithms (GA) is
            an idea that takes its inspiration from Darwinian evolution theory
            and employs operations such as "mutation" and "crossover" on
            candidate solutions, hypothesizing that two fit solutions are likely
            to have fit offspring. As it turns out GA is also a commonly used
            tool for optimizing machine learning training parameters, model
            architectures and for feature selection.
        </p>
        <p>
            A few weeks back and with all of the above in mind I wrote a simple
            event driven GA library with Postgres persistence for Rust. I will
            not be going in to the technical details of the library here as this
            chapter is about putting to the test. However, I welcome you to
            check it out, should you be interested:
        </p>
        <a href="https://github.com/jakob-lilliemarck/fx-durable-ga"
            >https://github.com/jakob-lilliemarck/fx-durable-ga
        </a>
        <p>The question on my mind was:</p>
        <blockquote>
            Could genetic algorithms be a way to search for "predictive ability"
            over feature selection, preprocessing, model architecture and
            training parameters combined?
        </blockquote>
        <p>
            Searching for a good solution across all of those, is after all the
            reality of training just about any model on any dataset, and surely
            that's a space is bound to be so large that its impractical,
            improbable and perhaps even impossible, for any manual process to
            find a global optimum, or even a good local one. In search of an
            answer to my question I set up a project to attempt a combined
            search for predictive ability using the "Beijing Multi-Site Air
            Quality" dataset (Chen, 2017) while making use of my new library
            <a href="https://github.com/jakob-lilliemarck/fx-durable-ga">
                fx-durable-ga </a
            >. You can find the project here:
        </p>
        <a href="https://github.com/jakob-lilliemarck/fx-fitness-search"
            >https://github.com/jakob-lilliemarck/fx-fitness-search
        </a>
    </section>

    <section>
        <h2>Genetic encoding and fitness</h2>
        <p>
            So how does one connect a genetic algorithm to a research question,
            and get it to search and find candidate solutions? Lets for a moment
            consider a much smaller and much simpler search space to illustrate
            this process.
        </p>
        <p>
            Lets imagine that we're searching for a point <var>A</var> within a
            cube. The cube can be represented using the cartesian coordinate
            system, within which it occupies some space along each of the three
            axis <var>X</var>, <var>Y</var> and <var>Z</var>. The geometric
            bounds of the cube could be defined as a maximum and a minimum value
            along each axis. For simplicities sake, let's say the cube occupies
            the space between the point <code>[0.0, 0.0, 0.0]</code> and
            <code>[1.0, 1.0, 1.0]</code>, the full cube could then be described
            as the following:
        </p>
        <pre><code class="language-javascript">[
  { min: 0.0, max: 1.0 }, // x_axis
  { min: 0.0, max: 1.0 }, // y_axis
  { min: 0.0, max: 1.0 }  // z_axis
]</code></pre>
        <p>
            That now defines all valid candidate solutions, the infinite number
            of points within the cube, and separates them from the much larger
            infinite number of invalid solutions outside of the cube. As such,
            the definition effectively captures the morphology of our candidate
            solution, the space we're searching within.
        </p>
        <p>
            We could then say that the fittest candidate solution (a point) will
            be at zero distance from <code>A</code>, the point we're search for.
            To compute the fitness of a candidate, we measure its distance to
            <code>A</code>. Points closer to <code>A</code> will have a lower
            fitness, while points further a away will have a higher fitness
            value - our optimization will strive to minimize fitness.
        </p>
        <p>
            Now we mostly have what we need to generate candidate solutions. For
            reasons beyond this example, the framework I've written requires
            genes to be integers, so in order to adapt this example and adhere
            to those requirements we'll need to also define the number of steps,
            the resolution, along each axis. In other words we'll need to
            discretize the space. Discretizing the search space also limits it
            to a finite number of solutions and makes the resolution explicit.
        </p>
        <p>
            Every valid point along an axis
            <code>{ min: 0.0, max: 1.0, steps: 11 }</code>
            could then be described as one of the 11 integer numbers between 0
            and 10. We now possess the means of encoding and decoding a point
            through the morphological bounds of the cube. We call the decoded
            form the phenotype, a term from genetics encompassing the observable
            traits of an organism, and the encoded form the genotype, the
            genetic information of that organism. To give an example:
        </p>
        <pre><code class="language-javascript">// Morphology, the space we search solutions within
[
  { min: 0.0, max: 1.0, steps: 11 },
  { min: 0.0, max: 1.0, steps: 11 },
  { min: 0.0, max: 1.0, steps: 11 }
]

// Point candidate solution, phenotype representation
{
  x: 0.2,
  y: 0.1,
  z: 0.9
}

// Candidate solution, encoded to its genotype representation
[
  2, // 0.2 * (11 - 1) = 0.2 * 10 = 2,
  1, // 0.1 * (11 - 1) = 0.1 * 10 = 1
  9  // 0.9 * (11 - 1) = 0.9 * 10 = 9
]

// Candidate solution, decoded to its phenotype representation
{
  x: 0.2, // 2 / (11 - 1) = 2 / 10 = 0.2
  y: 0.1, // 1 / (11 - 1) = 1 / 10 = 0.1
  z: 0.9  // 9 / (11 - 1) = 9 / 10 = 0.9
}</code></pre>
        <p>
            This business of encoding and decoding may feel like jumping through
            hoops. However this transformation allows us to represent both
            integer and decimal numbers, booleans and enumerations, which
            together allows for expressing most conceivable search spaces while
            allowing the framework to handle any gene of any search space in a
            uniform manner. Using the morphological definition, the GA framework
            can now generate, breed and mutate genomes as well as evaluating
            their fitness. That is the basics of a genetic algorithm
            optimization solver.
        </p>
    </section>

    <section>
        <h2>Putting it in context</h2>
        <p>Returning to the hypothesis:</p>
        <blockquote>
            Could genetic algorithms be a way to search for "predictive ability"
            over feature selection, preprocessing, model architecture and
            training parameters combined?
        </blockquote>
        <p>
            Looking at the dataset, and considering this question I drew a few
            conclusions:
        </p>
        <ul>
            <li>
                I wanted the algorithm to freely be able to select an arbitrary
                number of data points from the input dataset, optimizing feature
                selection.
            </li>
            <li>
                I wanted the algorithm to freely be able to select up to a
                predefined number of preprocessing methods for each data point,
                optimizing feature engineering.
            </li>
            <li>
                I wanted to optimize model architecture and training parameters
                together with with feature selection and feature engineering.
            </li>
        </ul>
            <p>
                The idea is simple. Only some combinations of features and
                preprocessor will provide the training loop with good learnable
                patterns while most will just be noise. But representing
                optional values within our GA poses a new challenge as the
                genotype encoding relies on the position of genes. So how to
                represent those "arbitrary number of" or "up to" expressions?
            </p>
            <p>
                What if we used 2 genes to "wrap" the genes encoding a specific
                attribute? Something like:
            </p>
            <ul>
                <li>
                    One gene bounded at 0 and 1 with 2 steps, encoding if the
                    wrapped "block" is active or not.
                </li>
                <li>
                    One gene bounded between 0 and the number of options (column
                    names, preprocessor types, etc.), acting like a "selector".
                </li>
            </ul>
<pre><code class="language-javascript">[
    // The on/off "block toggle"
    { min: 0.0, max: 1.0, steps: 2 },
    // The "column selector", there are 12 columns of interest.
    { min: 0.0, max: 11.0, steps: 12 },
    // .. the gene or genes wrapped by the toggle and the selector follow here
]</code></pre>
            <p>
                This design comes with the constraint that the wrapped genes
                must be of equal length and must share the same number of
                options per gene. While it didn't feel ideal, I decided it was
                good enough for now.
            </p>
        </ul>
    </section>
    <section>
        <h2>The experiment</h2>
        <p>TBD</p>
        <canvas id="chart-019af26e-80f7-7d23-bcb2-fe50cf9d52d8"></canvas>
<script>
    new Chart(document.getElementById('chart-019af26e-80f7-7d23-bcb2-fe50cf9d52d8'), {"type":"line","data":{"labels":["Jan","Feb","Mar"],"datasets":[{"label":"Sales","data":[65.0,59.0,80.0],"borderColor":"rgb(255, 0, 128)","tension":0.1}]}});
</script>
        <p>TBD</p>
    </section>
</article>

    </body>
</html>