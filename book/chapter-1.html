<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>In search of predictive ability</title>
        <style>
            :root {
                --primary: #b511d7; /*#4800b0 #ff0080*/
                --text: #222;
                --font-md: 1rem;
                --font-sm: 0.875rem;
                --spacing: 0.25rem;
            }
            html,
            body {
                font-size: var(--font-md);
                font-family:
                    -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                    "Helvetica Neue", Arial, sans-serif;
                line-height: 1.65;
                color: var(--text);
                display: flex;
                flex: 1 1;
                flex-direction: column;
                justify-content: center;
                margin: 0;
                padding: 0;
            }
            body {
                max-width: 80ch;
                margin: auto;
            }
            article {
                display: flex;
                flex-direction: column;
                padding: calc(16 * var(--spacing)) calc(8 * var(--spacing));
                counter-reset: conclusion;
            }
            article > header {
                margin-bottom: calc(8 * var(--spacing));
                padding-bottom: calc(4 * var(--spacing));
                display: flex;
                flex-direction: column;
            }
            .meta {
                display: flex;
                flex-direction: column;
                align-items: flex-end;
                gap: calc(1 * var(--spacing));
                margin-bottom: calc(2 * var(--spacing));
                font-size: var(--font-sm);
                color: #999;
                text-align: right;
            }
            .meta .author,
            .meta .date {
                display: block;
            }
            section {
                display: flex;
                flex-direction: column;
            }
            /* Add spacing between section elements */
            section > * + * {
                margin: 0;
                margin-top: calc(3 * var(--spacing));
            }
            /* Less spacing before blockquotes */
            section > blockquote {
                margin-top: calc(2 * var(--spacing));
            }
            section + section {
                border-top: 1px solid #eaeaea;
                margin-top: calc(8 * var(--spacing));
                padding-top: calc(6 * var(--spacing));
            }
            h1 {
                font-size: 2.5rem;
                line-height: 1.2;
                margin: 0;
                margin-bottom: calc(4 * var(--spacing));
            }
            h2 {
                font-size: 1.75rem;
                line-height: 1.3;
                margin: 0;
                margin-bottom: calc(2 * var(--spacing));
            }
            a {
                color: var(--primary);
                text-decoration: none;
            }
            a:hover {
                text-decoration: underline;
            }
            table {
                border-collapse: collapse;
                margin: 1rem 0;
            }
            th,
            td {
                border: 1px solid #ddd;
                padding: 0.5rem 0.75rem;
                text-align: left;
            }
            th {
                background-color: #f5f5f5;
                font-weight: bold;
            }
            blockquote {
                background-color: #f9f9f9;
                border-left: 4px solid var(--primary);
                margin: 1rem 0;
                padding: 1rem 1.5rem;
                font-style: italic;
            }
            .conclusion {
                position: relative;
                padding-left: 2ch;
                margin-left: 0;
            }
            .conclusion::before {
                counter-increment: conclusion;
                content: counter(conclusion);
                position: absolute;
                left: 0;
                top: 0;
                font-weight: 700;
                font-size: 1em;
                line-height: 1.65;
                color: var(--primary);
            }
            var,
            code {
                background-color: #f5f5f5;
                color: var(--primary);
                padding: 0.125rem 0.375rem;
                border-radius: var(--spacing);
                font-family: "Courier New", monospace;
                font-size: var(--font-sm);
            }
            pre {
                background-color: #f5f5f5;
                padding: 1rem;
                border-radius: calc(2 * var(--spacing));
                overflow-x: auto;
                margin: calc(3 * var(--spacing)) 0;
                box-sizing: border-box;
                max-width: 100%;
            }
            canvas {
                max-width: 100%;
            }
            pre code {
                background-color: transparent;
                color: inherit;
                padding: 0;
                border-radius: 0;
            }
            /* Mobile optimizations */
            @media (max-width: 768px) {
                :root {
                    --font-md: 0.9rem;
                    --font-sm: 0.8rem;
                }
                article {
                    padding: calc(8 * var(--spacing)) calc(2 * var(--spacing));
                }
                h1 {
                    font-size: 2rem;
                }
                h2 {
                    font-size: 1.5rem;
                }
                pre,
                code {
                    font-size: 0.75rem;
                }
                pre {
                    max-width: calc(100vw - calc(4 * var(--spacing)));
                }
            }
        </style>
        <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
        <!-- For Prism.js with Night Owl theme -->
        <link
            href="https://cdn.jsdelivr.net/npm/prism-themes@1.9.0/themes/prism-night-owl.min.css"
            rel="stylesheet"
        />
        <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    </head>
    <body>
        
<article>
    <header>
        <div class="meta">
            <span class="author">Jakob Lilliemarck</span>
            <span class="date">December 2025</span>
        </div>
    </header>
    <section>
        <h1>In search of predictive ability</h1>
        <p>
            I've spent the better part of this Autumn thinking about and
            experimenting with machine learning in Rust. Initially I was curious
            if it would be feasible for me to do, and what it would take in
            terms of computational, not to mention cognitive resources - would
            it be within my reach?
        </p>
        <p>
            "Why Rust?", you may ask. Why not Python, or even C? I have been
            asking myself the same question, but truth be told, I find neither
            of which compelling to work with. After all, in most systems (yes,
            even machine learning ones) the majority of the code is not about
            tensors and models, but about data retrieval and processing. If you
            ask me, Rust has it all - excellent support for test driven
            development, memory safety, robust and explicit parallel computing
            and a proper expressive type system. I'd even argue that Rust is a
            simple language because in Rust there are no surprises and there is
            no magic.
        </p>
    </section>

    <section>
        <h2>The Morphological Space of Neural Networks</h2>
        <p>
            I've learned that while doing machine learning, there are a lot of
            decisions to make. How to handle missing data? How to process,
            extract and structure data in a form the models and frameworks will
            accept? Which model, architecture and training parameters to use,
            and for which use-case? The list goes on.
        </p>
        <p>
            From memory, most examples or articles I've seen in the past about
            machine learning (though I must admit I really haven't done my
            homework here) typically involve an enthusiastic author reasoning
            about the validity and meaning of parameters while making one-off
            experiments. That makes me itch. If there's anything equivalent to
            "code smell" for data science, I think that's it—you've got these
            sophisticated models producing measurable predictions, and yet you
            allow your human superstition and poor analytical skills to dictate
            the conditions of your experiments. I can't but ask "why?".
        </p>
        <p>
            Consider a simple model, a feedforward neural network. Even this
            seemingly straightforward architecture exists within a vast space of
            design choices. To illustrate the scale of this space, its
            "morphology" if you will, lets roughly discretize the key dimensions
            that impact model performance:
        </p>
        <table>
            <thead>
                <tr>
                    <th>Variable</th>
                    <th>Choices</th>
                    <th>Count</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Sequence length</td>
                    <td>5, 10, 20, 30, 50</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Hidden size</td>
                    <td>32, 64, 128, 256, 512</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Activation function</td>
                    <td>ReLU, Tanh, Sigmoid</td>
                    <td>3</td>
                </tr>
                <tr>
                    <td>Epochs</td>
                    <td>10, 25, 50, 100, 200</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Batch size</td>
                    <td>16, 32, 64, 128, 256</td>
                    <td>5</td>
                </tr>
                <tr>
                    <td>Learning rate</td>
                    <td>1e-4, 1e-3, 1e-2, 1e-1</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Optimizer type</td>
                    <td>Adam, SGD, RMSprop</td>
                    <td>3</td>
                </tr>
                <tr>
                    <td>Gradient clipping</td>
                    <td>None, 0.5, 1.0, 2.0</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Weight decay</td>
                    <td>None, 1e-4, 1e-3, 1e-2</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Patience</td>
                    <td>None, 5, 10, 20</td>
                    <td>4</td>
                </tr>
                <tr>
                    <td>Model initialization</td>
                    <td>Default, Xavier, He</td>
                    <td>3</td>
                </tr>
            </tbody>
        </table>
        <p>
            That is
            <code>5 × 5 × 3 × 5 × 5 × 4 × 3 × 4 × 4 × 4 × 3 = ~864,000</code>
            possible configurations before even considering the input data or
            its preprocessing!
        </p>
        <p>
            Even for a simple feedforward network, the morphological space is
            massive! Any approach relying on manual experimentation to search
            such is going to be impractical at best - this is a problem that
            calls for a structured quantitative approach.
        </p>
    </section>

    <section>
        <h2>Making sense of the mess</h2>
        <p>
            In a previous job of mine, I worked in an innovation hub at one of
            the larger architectural firms in Sweden. My role at the time was as
            <i>computational designer</i>, an odd title that often provided
            people with more questions than answers. As a title it was not
            ideal, but it was an interesting job. It involved applying
            programming, algorithms and systems thinking to the processes of
            creative exploration and manufacturing of complex three dimensional
            forms. The tool I used allowed for geometric optimization by means
            of a built in genetic algorithm solver. Genetic algorithms (GA) is
            an idea that takes its inspiration from Darwinian evolution theory
            and employs operations such as "mutation" and "crossover" to find
            good candidate solutions. The foundational hypothesis is that two
            fit solutions are likely to have fit offspring. As it turns out GA
            is also a commonly used tool for optimizing machine learning
            systems.
        </p>
        <p>
            A few weeks back, and with all of the above in mind, I wrote a
            simple event driven GA library with Postgres persistence for Rust. I
            will not be going in to the technical details of it now, however I
            welcome you to check it out, should you be interested:
        </p>
        <a href="https://github.com/jakob-lilliemarck/fx-durable-ga"
            >https://github.com/jakob-lilliemarck/fx-durable-ga
        </a>
        <p>The question on my mind was:</p>
        <blockquote>
            Could genetic algorithms be a way to search for "predictive ability"
            over feature selection, preprocessing, model architecture and
            training parameters combined?
        </blockquote>
        <p>
            Searching for a good solution across all of those is after all the
            reality of training just about any model, on any dataset. Surely
            that space is bound to be so large that it's impractical, improbable
            and perhaps even impossible, for any manual process to find a global
            optimum, or even a good local one. As a developer I've come to know
            my human weaknesses well enough that I do not want to pick those
            parameters by hand, but perhaps more importantly: I do not care
            about the parameters, I care about the performance of the system!
        </p>
        <p>
            In search of an answer to my question I set up a project to attempt
            a combined search for predictive ability using the "Beijing
            Multi-Site Air Quality" dataset (Chen, 2017) while making use of my
            new library:
        </p>
        <a href="https://github.com/jakob-lilliemarck/fx-fitness-search"
            >https://github.com/jakob-lilliemarck/fx-fitness-search
        </a>
    </section>

    <section>
        <h2>Genetic encoding and fitness</h2>
        <p>
            So how does one get a genetic algorithm to explore a landscape of
            solutions?
        </p>
        <p>
            In an attempt to shed some light, lets for a moment consider a much
            smaller and much simpler problem. Let's imagine that we're searching
            for a point <var>A</var> within a cube. The closer we get to the
            point, the better is our solution.
        </p>
        <p>
            We represent the cube using the cartesian coordinate system, within
            which it occupies some space along each of the three axes
            <var>X</var>, <var>Y</var> and <var>Z</var>. The geometric bounds of
            the cube could be defined as a maximum and a minimum value along
            each axis. For simplicities sake, let's say the cube occupies the
            space between the origin point <code>[0.0, 0.0, 0.0]</code> and a
            point <code>[1.0, 1.0, 1.0]</code>. The full cube could then be
            represented as the following.
        </p>
        <pre><code class="language-javascript">// X, Y and Z axis
[
  { min: 0.0, max: 1.0 },
  { min: 0.0, max: 1.0 },
  { min: 0.0, max: 1.0 }
]</code></pre>
        <p>
            Knowing that we're searching for a point <i>within</i> the cube we
            can now discard the infinite numbers of points that lie outside it,
            and focus our attention to the much smaller infinite number of
            points that lie within it. In other words, we know the valid ranges
            of values for <var>X</var>, <var>Y</var> and <var>Z</var>, those
            define the <i>morphology</i> of our search space.
        </p>
        <p>
            To compute the <i>fitness</i> of a candidate, we measure its
            distance to point <code>A</code>. Points closer to
            <code>A</code> will have a lower fitness, while points further a
            away will have a higher fitness. Our optimization will strive to
            minimize fitness.
        </p>
        <p>
            Now we mostly have what we need to generate candidate solutions. For
            reasons beyond this example, the framework I've written requires
            genes to be integers, so in order to adapt this example and adhere
            to those requirements we'll also need to define the number of steps,
            the resolution, along each axis. That limits the search space
            further, makes the resolution explicit and the number of
            configuration finite.
        </p>
        <p>
            Every valid point along an axis like
            <code>{ min: 0.0, max: 1.0, steps: 11 }</code>
            could then be described as one of the 11 integer numbers between 0
            and 10. We now possess the means of encoding and decoding a point
            through the morphological bounds of the cube. We call the decoded
            form the <i>phenotype</i>, a term from genetics encompassing the
            observable traits of an organism, and the encoded form the
            <i>genotype</i>, the genetic information of that organism. To give
            an example:
        </p>
        <pre><code class="language-javascript">// Morphology, the space we search solutions within
[
  { min: 0.0, max: 1.0, steps: 11 },
  { min: 0.0, max: 1.0, steps: 11 },
  { min: 0.0, max: 1.0, steps: 11 }
]

// Point candidate solution, phenotype representation
{
  x: 0.2,
  y: 0.1,
  z: 0.9
}

// Candidate solution, encoded to genotype representation
[
  2, // 0.2 * (11 - 1) = 0.2 * 10 = 2,
  1, // 0.1 * (11 - 1) = 0.1 * 10 = 1
  9  // 0.9 * (11 - 1) = 0.9 * 10 = 9
]</code></pre>
        <p>
            This business of encoding and decoding may feel like jumping through
            hoops. However this transformation allows us to represent both
            integer and decimal numbers, booleans and enumerations, which
            together allows for expressing most conceivable search spaces while
            allowing the framework to handle any gene of any search space in a
            uniform manner. Using the morphological definition, the GA framework
            can now <code>generate</code>, <code>breed</code>,
            <code>mutate</code> and <code>evaluate</code> phenotypes. That is
            the foundation of a GA optimization solver.
        </p>
    </section>

    <section>
        <h2>Putting it in context</h2>
        <p>
            Looking at the dataset and considering my question I drew three
            conclusions.
        </p>
        <p class="conclusion">
            I do not know which data points have a strong or weak correlation
            and which would provide valuable information and which would just
            provide noise. Therefore I want the system to freely optimize the
            selection of data points, to include an arbitrary number from those
            available in the dataset.
        </p>
        <p class="conclusion">
            I can not determine which preprocessing methods are suitable to
            extract information from which data points. Therefore I want the
            system to freely optimize the selection and preprocessing methods
            and their parameters for each selected data point.
        </p>
        <p class="conclusion">
            The relationships between the selection of data points,
            preprocessors, training parameters and model architecture may
            contain valuable recognizable patterns. Any attempt at a stepwise
            optimization, for example first optimizing model architecture, and
            then optimizing training parameters, will hide these patterns from
            the system. Therefore I want to optimize them all at once, in a
            <i>combined</i> search for fitness.
        </p>
        <p>
            But representing optional values within our GA poses a new challenge
            as the genotype encoding relies on the position of genes. So how to
            represent those "arbitrary number of" or "up to" expressions?
        </p>
        <p>
            What if we used 2 genes to "wrap" the genes encoding a specific
            attribute? Something like:
        </p>
        <ul>
            <li>
                One gene bounded at 0 and 1 with 2 steps, encoding if the
                wrapped "block" is active or not.
            </li>
            <li>
                One gene bounded between 0 and the number of options (column
                names, preprocessor types, etc.), acting like a "selector".
            </li>
        </ul>
        <pre><code class="language-javascript">[
    // The on/off "block toggle"
    { min: 0.0, max: 1.0, steps: 2 },
    // The "column selector", there are 12 columns of interest
    { min: 0.0, max: 11.0, steps: 12 },
    // .. the wrapped gene(s) follow here
]</code></pre>
        <p>
            This design comes with the constraint that the wrapped genes must be
            of equal length and must share the same number of options per gene.
            While that's far from ideal I decided it was good enough for now.
        </p>
    </section>
    <section>
        <h2>Putting it all together</h2>
        <p>
            So, does this complex and computationally expensive approach hold
            any merit? Will it be able to find any promising configurations
            within this immense search space?
        </p>
        <p>
            I initiated an optimization using the command below, basing it on
            some past experience that for a given number of evaluations, a
            higher number of generations typically produce better results by
            providing more exploitation.
        </p>
        <pre><code class="language-bash">target/release/client beijing request-optimization \
  --fitness-goal 'MIN(0.0)' \
  --schedule 'GENERATIONAL(100, 12)' \
  --selector 'TOURNAMENT(3, 12)' \
  --mutagen 'MUTAGEN(0.7, 0.4)' \
  --initial-population 50 \
  --prediction-horizon 1 \
  --epochs 20 \
  --patience 3 \
  --validation-start-epoch 5 \
  --batch-size 128</code></pre>
        <p></p>
        <canvas id="chart-019bb12d-dfdb-7292-92ba-8b5fceb06284"></canvas>
<script>
    new Chart(document.getElementById('chart-019bb12d-dfdb-7292-92ba-8b5fceb06284'), {"type":"line","data":{"labels":["Gen 1","Gen 2","Gen 3","Gen 4","Gen 5","Gen 6","Gen 7","Gen 8","Gen 9","Gen 10","Gen 11","Gen 12","Gen 13","Gen 14","Gen 15","Gen 16","Gen 17","Gen 18","Gen 19","Gen 20","Gen 21","Gen 22","Gen 23","Gen 24","Gen 25","Gen 26","Gen 27","Gen 28","Gen 29","Gen 30","Gen 31","Gen 32","Gen 33","Gen 34","Gen 35","Gen 36","Gen 37","Gen 38","Gen 39","Gen 40","Gen 41","Gen 42","Gen 43","Gen 44","Gen 45","Gen 46","Gen 47","Gen 48","Gen 49","Gen 50","Gen 51","Gen 52","Gen 53","Gen 54","Gen 55","Gen 56","Gen 57","Gen 58","Gen 59","Gen 60","Gen 61","Gen 62","Gen 63","Gen 64","Gen 65","Gen 66"],"datasets":[{"label":"Average Fitness","data":[20.18,30.51,23.5,24.61,16.96,56.75,18.96,14.2,45.58,18.63,33.85,39.67,12.79,24.11,11.71,27.01,12.6,23.62,12.47,40.84,25.28,11.29,21.81,15.99,10.63,59.16,45.02,42.99,15.92,37.1,48.19,27.58,37.36,25.83,10.37,27.1,26.56,13.24,47.14,12.38,12.61,24.66,21.43,28.1,26.77,26.4,50.82,22.71,27.5,30.8,23.83,33.96,37.52,20.67,14.63,9.88,11.82,29.45,47.54,20.65,70.05,24.03,22.86,57.19,13.26,12.3],"borderColor":"rgb(75, 192, 192)","tension":0.1,"pointRadius":0.0,"borderWidth":1.5},{"label":"Trend: Average Fitness","data":[25.5265,25.576002,25.625504,25.675005,25.724506,25.77401,25.823511,25.873013,25.922514,25.972015,26.021517,26.071018,26.12052,26.170021,26.219522,26.269026,26.318527,26.368029,26.41753,26.467031,26.516533,26.566034,26.615536,26.665037,26.714539,26.764042,26.813543,26.863045,26.912546,26.962048,27.011549,27.06105,27.110552,27.160053,27.209557,27.259058,27.30856,27.35806,27.407562,27.457064,27.506565,27.556067,27.605568,27.655071,27.70457,27.754074,27.803576,27.853077,27.902578,27.95208,28.001581,28.051083,28.100584,28.150085,28.199589,28.24909,28.298592,28.348093,28.397594,28.447096,28.496597,28.546099,28.5956,28.645103,28.694605,28.744106],"borderColor":"rgb(255, 99, 132)","tension":0.0,"pointRadius":0.0,"borderWidth":1.0}]},"options":{"animation":false,"aspectRatio":1.7}});
</script>
        <p>TBD</p>
        <pre><code class="language-bash">target/release/client beijing request-optimization \
  --fitness-goal 'MIN(0.0)' \
  --schedule 'GENERATIONAL(100, 48)' \
  --selector 'TOURNAMENT(5, 400)' \
  --mutagen 'MUTAGEN(0.3, 0.15)' \
  --initial-population 200 \
  --prediction-horizon 1 \
  --epochs 30 \
  --patience 3 \
  --validation-start-epoch 5 \
  --batch-size 128</code></pre>
        <p>TBD</p>
    </section>
</article>

    </body>
</html>